{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A6Q701KCMYox",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab1e512b-a4b3-4592-c84a-2e8adf2e6221"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.1.tar.gz (317.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.0/317.0 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.1-py2.py3-none-any.whl size=317488493 sha256=a7b5c0b0418458bff0cbd28581c7866e2e97a670954458278c9b230e0b5bc887\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/1d/60/2c256ed38dddce2fdd93be545214a63e02fbd8d74fb0b7f3a6\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.1\n"
          ]
        }
      ],
      "source": [
        "pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import pandas as pd"
      ],
      "metadata": {
        "id": "CbehywgcQwfh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#df=pd.read_csv('/content/drive/MyDrive/Datasets/salestxns.csv')"
      ],
      "metadata": {
        "id": "sc4jer7fQ2rc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#df.head()"
      ],
      "metadata": {
        "id": "ElqlEFzzQ8b-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#dff=pd.read_csv('/content/drive/MyDrive/Datasets/customers.csv')"
      ],
      "metadata": {
        "id": "PTW54R-yST93"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#dff.head()"
      ],
      "metadata": {
        "id": "xZoJXNVhSUAa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, sum\n",
        "\n",
        "# Initialize Spark session\n",
        "spark = SparkSession.builder.appName(\"CustomerSalesAggregation\").getOrCreate()\n",
        "\n",
        "# Step 1: Load Customer Data\n",
        "df = spark.read \\\n",
        "    .format(\"csv\") \\\n",
        "    .option(\"header\", True) \\\n",
        "    .load(\"/content/drive/MyDrive/Datasets/customers.csv\")\n",
        "\n",
        "# Step 2: Broadcast Customer Data\n",
        "broadcast_customer_data = spark.sparkContext.broadcast(\n",
        "    df.select(\"Customer_Id\", \"Name\").toPandas()\n",
        ")\n",
        "\n",
        "# Step 3: Load Sales Transaction Data (Assuming you have a sales DataFrame named sales_df)\n",
        "sales_df = spark.read \\\n",
        "    .format(\"csv\") \\\n",
        "    .option(\"header\", True) \\\n",
        "    .load(\"/content/drive/MyDrive/Datasets/salestxns.csv\")\n",
        "\n",
        "# # Step 4: Join Sales Transaction Data with Broadcasted Customer Data\n",
        "joined_df = sales_df.join(broadcast_customer_data.value, col(\"Customer_Id\") == col(\"Customer_Id\"))\n",
        "\n",
        "# # Step 5: Perform Aggregations\n",
        "aggregated_df = joined_df.groupBy(\"Customer_Id\", \"Name\", \"Product_Id\", \"Product_Name\", \"Price\") \\\n",
        "    .agg(\n",
        "        sum(\"Qty\").alias(\"Total_Quantity\"),\n",
        "        sum(\"Price\").alias(\"Total_Amount_Paid\")\n",
        "    )\n",
        "\n",
        "# Step 6: Display the Result\n",
        "aggregated_df.show(truncate=False)\n",
        "\n",
        "# Step 7: Stop Spark Session\n",
        "spark.stop()"
      ],
      "metadata": {
        "id": "QLciqJTYMcfD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "88e1813f-0dcf-4f68-db47-64be8aa831fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'DataFrame' object has no attribute '_jdf'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-2f3fb8525f7d>\u001b[0m in \u001b[0;36m<cell line: 25>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# # Step 4: Join Sales Transaction Data with Broadcasted Customer Data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mjoined_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msales_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbroadcast_customer_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Customer_Id\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Customer_Id\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m# # Step 5: Perform Aggregations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, other, on, how)\u001b[0m\n\u001b[1;32m   2489\u001b[0m                 \u001b[0mon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jseq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2490\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhow\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"how should be a string\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2491\u001b[0;31m             \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2492\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparkSession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   5900\u001b[0m         ):\n\u001b[1;32m   5901\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5902\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5903\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5904\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute '_jdf'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#customer_df"
      ],
      "metadata": {
        "id": "Rq_OpeNYTpPh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = spark.read \\\n",
        "    .format(\"csv\") \\\n",
        "    .option(\"header\", True) \\\n",
        "    .load(\"/content/drive/MyDrive/Datasets/customers.csv\")"
      ],
      "metadata": {
        "id": "6g4CeIlbMch_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df"
      ],
      "metadata": {
        "id": "e_ddA5_yMclC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a7b2b4b-e49b-4814-a937-81e2f9b244fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[Customer_Id: string, Name: string, City: string, State: string, Zipcode: string]"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ef4NATW7WIov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"CSVReader\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "df = spark.read \\\n",
        "    .format(\"csv\") \\\n",
        "    .option(\"header\", True) \\\n",
        "    .option(\"delimiter\", \",\") \\\n",
        "    .load(\"/content/drive/MyDrive/Datasets/customers.csv\")\n",
        "\n",
        "df.show(5)\n",
        "\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1hvuUtF4WIra",
        "outputId": "78d74a04-0f36-434b-b47d-56ad195cdaa7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+--------------+---------+-----+-------+\n",
            "|Customer_Id|          Name|     City|State|Zipcode|\n",
            "+-----------+--------------+---------+-----+-------+\n",
            "|      11039|   Mary Torres|   Caguas|   PR|    725|\n",
            "|       5623|    Jose Haley| Columbus|   OH|  43207|\n",
            "|       5829|    Mary Smith|  Houston|   TX|  77015|\n",
            "|       6336|Richard Maddox|   Caguas|   PR|    725|\n",
            "|       1708|Margaret Booth|Arlington|   TX|  76010|\n",
            "+-----------+--------------+---------+-----+-------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UT50S0C9XrR1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_p-JzcCeXrUh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, sum\n",
        "\n",
        "# Initialize Spark session\n",
        "spark = SparkSession.builder.appName(\"CustomerSalesAggregation\").getOrCreate()\n",
        "\n",
        "# Step 1: Load Customer Data\n",
        "customer_df = spark.read \\\n",
        "    .format(\"csv\") \\\n",
        "    .option(\"header\", True) \\\n",
        "    .load(\"/content/drive/MyDrive/Datasets/customers.csv\")\n",
        "\n",
        "# Step 2: Create a lookup table (optimized for joins)\n",
        "customer_lookup = customer_df.select(\"Customer_Id\", col(\"Name\").alias(\"Customer_Name\")).cache()\n",
        "\n",
        "# Step 3: Load Sales Transaction Data\n",
        "sales_df = spark.read \\\n",
        "    .format(\"csv\") \\\n",
        "    .option(\"header\", True) \\\n",
        "    .load(\"/content/drive/MyDrive/Datasets/salestxns.csv\")\n",
        "\n",
        "# Step 4: Join using the lookup table (avoiding broadcast)\n",
        "joined_df = sales_df.join(customer_lookup, on=\"Customer_Id\")\n",
        "\n",
        "# Step 5: Perform Aggregations\n",
        "# aggregated_df = joined_df.groupBy(\"Customer_Id\", \"Customer_Name\", \"Product_Id\", \"Product_Name\", \"Price\") \\\n",
        "#     .agg(\n",
        "#         sum(\"Qty\").alias(\"Total_Quantity\"),\n",
        "#         sum(\"Price\").alias(\"Total_Amount_Paid\")\n",
        "#     )\n",
        "aggregated_df = joined_df.groupBy(\"Customer_Id\", \"Customer_Name\", \"Product_Id\", \"Product_Name\", \"Price\") \\\n",
        "    .agg(\n",
        "        sum(\"Qty\").alias(\"Total_Quantity\"),\n",
        "        sum(col(\"Qty\") * col(\"Price\")).alias(\"Total_Amount_Paid\")\n",
        "    )\n",
        "\n",
        "# Step 6: Display the Result\n",
        "aggregated_df.show(truncate=False)\n",
        "\n",
        "# Step 7: Stop Spark Session\n",
        "spark.stop()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z6b9uxwUXrg0",
        "outputId": "4bdf4e32-aa2c-4ce2-e6a8-bffb9e78d84d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+----------------+----------+---------------------------------------------+------+--------------+------------------+\n",
            "|Customer_Id|Customer_Name   |Product_Id|Product_Name                                 |Price |Total_Quantity|Total_Amount_Paid |\n",
            "+-----------+----------------+----------+---------------------------------------------+------+--------------+------------------+\n",
            "|7491       |Sharon Dominguez|502       |Nike Men's Dri-FIT Victory Golf Polo         |50    |11.0          |550.0             |\n",
            "|1518       |Jessica Smith   |191       |Nike Men's Free 5.0+ Running Shoe            |99.99 |14.0          |1399.86           |\n",
            "|11845      |Mary Weaver     |1014      |O'Brien Men's Neoprene Life Vest             |49.98 |4.0           |199.92            |\n",
            "|712        |Mary Ayala      |1004      |Field & Stream Sportsman 16 Gun Fire Safe    |399.98|2.0           |799.96            |\n",
            "|8757       |Tyler Gutierrez |627       |Under Armour Girls' Toddler Spine Surge Runni|39.99 |7.0           |279.93            |\n",
            "|2420       |Mary Smith      |1004      |Field & Stream Sportsman 16 Gun Fire Safe    |399.98|3.0           |1199.94           |\n",
            "|10447      |Mary Smith      |502       |Nike Men's Dri-FIT Victory Golf Polo         |50    |27.0          |1350.0            |\n",
            "|12410      |Bryan Smith     |502       |Nike Men's Dri-FIT Victory Golf Polo         |50    |8.0           |400.0             |\n",
            "|660        |Shirley Baker   |1073      |Pelican Sunstream 100 Kayak                  |199.99|1.0           |199.99            |\n",
            "|245        |Deborah Smith   |957       |Diamondback Women's Serene Classic Comfort Bi|299.98|1.0           |299.98            |\n",
            "|9285       |Gloria Smith    |235       |Under Armour Hustle Storm Medium Duffle Bag  |34.99 |5.0           |174.95000000000002|\n",
            "|4482       |Randy Elliott   |365       |Perfect Fitness Perfect Rip Deck             |59.99 |9.0           |539.91            |\n",
            "|11089      |Mary Smith      |1014      |O'Brien Men's Neoprene Life Vest             |49.98 |7.0           |349.86            |\n",
            "|5253       |Susan Watkins   |502       |Nike Men's Dri-FIT Victory Golf Polo         |50    |1.0           |50.0              |\n",
            "|10508      |Angela Meadows  |502       |Nike Men's Dri-FIT Victory Golf Polo         |50    |10.0          |500.0             |\n",
            "|1598       |Mary Green      |403       |Nike Men's CJ Elite 2 TD Football Cleat      |129.99|4.0           |519.96            |\n",
            "|11526      |Dorothy Crawford|365       |Perfect Fitness Perfect Rip Deck             |59.99 |10.0          |599.9             |\n",
            "|11681      |Roy Jackson     |365       |Perfect Fitness Perfect Rip Deck             |59.99 |10.0          |599.9             |\n",
            "|9119       |Mary Smith      |1014      |O'Brien Men's Neoprene Life Vest             |49.98 |3.0           |149.94            |\n",
            "|8408       |Patrick Smith   |1073      |Pelican Sunstream 100 Kayak                  |199.99|2.0           |399.98            |\n",
            "+-----------+----------------+----------+---------------------------------------------+------+--------------+------------------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    }
  ]
}